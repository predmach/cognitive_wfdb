{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_data_directories(aRawDirectory, aStageDirectory, aDataStoreName):\n",
    "    '''\n",
    "\n",
    "    Args:\n",
    "        aRawDirectory : where to store the downloaded data\n",
    "        aStageDirectory : where to convert data to CSV & TXT\n",
    "        aDataStoreName : where to establish the HDF5 DataStore\n",
    "\n",
    "    Returns:\n",
    "        none\n",
    "    '''\n",
    "    # shutil.rmtree(aRawDirectory,True)\n",
    "    # shutil.rmtree(aStageDirectory,True)\n",
    "    # shutil.rmtree(aDataStoreName,True)\n",
    "    import errno\n",
    "    def mkdir_p(path):\n",
    "        try:\n",
    "            os.makedirs(path)\n",
    "        except OSError as exc:  # Python >2.5\n",
    "            if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
    "                pass\n",
    "            else:\n",
    "                raise\n",
    "    mkdir_p(aRawDirectory)\n",
    "    mkdir_p(aStageDirectory)\n",
    "    mkdir_p(aDataStoreName)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note \n",
    "apt install python3-lxml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): lxml in /opt/conda/lib/python3.5/site-packages\r\n",
      "\u001b[33mYou are using pip version 8.1.2, however version 9.0.1 is available.\r\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def download_physionet_files(aDatabase='mghdb', aTargetDataDirectory='./data', shouldClean=False,\n",
    "                             useExtensions=['atr', 'dat', 'hea']):\n",
    "    \n",
    "    '''\n",
    "    Download MGH/MF data files from Physionet\n",
    "\n",
    "    Args:\n",
    "        aTargetDataDirectory : directory in which to store raw files\n",
    "        aDatabase\t\t: default 'mghdb'\n",
    "        shouldClean\t\t: reload the data?\n",
    "        useExtensions\t: different databases will have different files ['atr','dat','hea']\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    \n",
    "    extensionRegex = ''\n",
    "    for ext in useExtensions:\n",
    "        if len(extensionRegex) > 1:\n",
    "            extensionRegex += '|'\n",
    "        extensionRegex = extensionRegex + '\\.' + ext\n",
    "\n",
    "    urlPhysionetDB = 'https://www.physionet.org/physiobank/database/' + aDatabase + '/'\n",
    "    htmlDB = requests.get(urlPhysionetDB).content\n",
    "\n",
    "    # Scrape the list of all data files out of database page\n",
    "    scraper = BeautifulSoup(htmlDB, \"lxml\")\n",
    "    hrefElements = [pageElement['href'] for pageElement in scraper.find_all('a', href=True)]\n",
    "    dataElements = filter(lambda pageElement: re.search(extensionRegex, pageElement, re.I), hrefElements)\n",
    "    dataElements = sorted(dataElements)\n",
    "    downloadURLList = [urlPhysionetDB + dataLink for dataLink in dataElements]\n",
    "\n",
    "    targetFileList = [os.path.join(aTargetDataDirectory, fileName) for fileName in dataElements]\n",
    "    i = 0\n",
    "    for dataURL, localDataFile in zip(downloadURLList, targetFileList):\n",
    "        i += 1\n",
    "        #\t\tif i > 10*3:\n",
    "        #\t\t\tbreak\n",
    "\n",
    "        if os.path.isfile(localDataFile) and not shouldClean:  # don't re-download\n",
    "            continue\n",
    "\n",
    "        print('downloading {0}'.format(localDataFile))\n",
    "        with open(localDataFile, 'w+b') as localFileHandle:\n",
    "            localFileHandle.write(requests.get(dataURL).content)\n",
    "\n",
    "def download_annotation_metadata():\n",
    "    '''\n",
    "    Args:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        DataFrame\n",
    "    '''\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    import pandas as pd\n",
    "\n",
    "    # Scrape annotation table frm MIT_-BIH info page\n",
    "    annotationURL = 'https://www.physionet.org/physiobank/database/html/mitdbdir/intro.htm#annotations'\n",
    "    htmlAnnotations = requests.get(annotationURL).content\n",
    "    scraper = BeautifulSoup(htmlAnnotations, \"lxml\")\n",
    "    tableElements = scraper.select('table')[-1]\n",
    "\n",
    "    #\n",
    "    metadata = pd.read_html(str(tableElements), header=0)[0]\n",
    "    mask = metadata.Symbol.apply(lambda x: True)\n",
    "    mask.ix[[20, 36]] = False\n",
    "    metadata = metadata[mask]\n",
    "    metadata['Symbol'] = metadata['Symbol'].astype(str)\n",
    "\n",
    "    metadata.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    metadata.loc[0, 'Symbol'] = 'N'\n",
    "    metadata.loc[37, 'Symbol'] = 'M'\n",
    "    metadata.loc[38, 'Symbol'] = 'P'\n",
    "    metadata.loc[39, 'Symbol'] = 'T'\n",
    "\n",
    "    lut = {\n",
    "        'artifact': ['artifact', 'Unclassified', 'Non-conducted', 'Fusion'],\n",
    "        'arrythmia': ['flutter', 'bigeminy', 'tachycardia', 'fibrillation'],\n",
    "        'other': ['bradycardia', 'Abberated', 'Premature', 'escape'],\n",
    "        'signal': ['Signal quality', 'Extreme noise', 'Missed beat', 'Pause', 'Tape slippage']\n",
    "    }\n",
    "    def has_words(aTestString, aKeyList):\n",
    "        for keyString in aKeyList:\n",
    "            if not aTestString.find(keyString) == -1:\n",
    "                # if keyString.lower() in aTestString.lower():\n",
    "                return True\n",
    "        return False\n",
    "    for i in lut.keys():\n",
    "        metadata[i] = metadata.Meaning.apply(lambda x: has_words(x, lut[i]))\n",
    "\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_physionet_data_to_csv(aDatabase, aSourceDirectory, aTargetDirectory, shouldClean=False):\n",
    "    '''\n",
    "    Convert raw data to CSV & TXT\n",
    "\n",
    "    Args:\n",
    "        aSourceDirectory :\n",
    "        aTargetDirectory :\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    from subprocess import Popen, PIPE\n",
    "    \n",
    "    rawDataFiles = set()\n",
    "    for dataFileName in os.listdir(aSourceDirectory):\n",
    "        found = re.search('^(\\d\\d\\d)\\.', dataFileName)\n",
    "        if found:\n",
    "            rawDataFiles.add(found.group(1))\n",
    "\n",
    "    conversionProcesses = set()\n",
    "    targetSampleFile = aTargetDirectory + '/' + aDatabase + '.{name}.csv'\n",
    "    convertSamples = 'rdsamp -r ' + aSourceDirectory + '/{name} -c -v -pe > {stdout}'\n",
    "    targetAnnotationFile = aTargetDirectory + '/' + aDatabase + '.ann.{name}.txt'\n",
    "    convertAnnotation = 'rdann -r ' + aSourceDirectory + '/{name} -a atr -v -e > {stdout}'\n",
    "\n",
    "    maxOpenFiles = 6\n",
    "    lowOpenFiles = 3\n",
    "    numOpenFiles = 0\n",
    "\n",
    "    for rawDataFile in rawDataFiles:\n",
    "\n",
    "        targetSample = targetSampleFile.format(name=rawDataFile)\n",
    "        if not os.path.isfile(targetSample) and not shouldClean:\n",
    "            print(targetSample)\n",
    "            sampleProcess = Popen(convertSamples.format(name=rawDataFile, stdout=targetSample), shell=True, stdout=PIPE)\n",
    "            conversionProcesses.add(sampleProcess)\n",
    "            numOpenFiles += 1\n",
    "\n",
    "        targetAnnotation = targetAnnotationFile.format(name=rawDataFile)\n",
    "        if not os.path.isfile(targetAnnotation) and not shouldClean:\n",
    "            annotationProcess = Popen(convertAnnotation.format(name=rawDataFile, stdout=targetAnnotation), shell=True,\n",
    "                                      stdout=PIPE)\n",
    "            conversionProcesses.add(annotationProcess)\n",
    "            numOpenFiles += 1\n",
    "\n",
    "        if numOpenFiles > maxOpenFiles:\n",
    "            print('Reached max processes {0} pending'.format(numOpenFiles))\n",
    "            for conversionProcess in conversionProcesses:\n",
    "                numOpenFiles -= 1\n",
    "                if numOpenFiles == lowOpenFiles:\n",
    "                    break\n",
    "                conversionProcess.communicate()\n",
    "            conversionProcesses.clear()\n",
    "            numOpenFiles = 0\n",
    "\n",
    "    for conversionProcess in conversionProcesses:\n",
    "        conversionProcess.communicate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_hdf5_data_store(aSourceDataDirectory, aTargetHDF5):\n",
    "    '''\n",
    "    writes all mitdb source directory csvs and txt annotations to a single hdf5 file\n",
    "\n",
    "    Args:\n",
    "        aSourceDataDirectory: full path of directory containing mitdb csvs and txt annotations\n",
    "        aTargetHDF5: fullpath of target file\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    import pandas as pd\n",
    "\n",
    "    allRawDataFiles = os.listdir(aSourceDataDirectory)\n",
    "\n",
    "    sampleFiles = filter(lambda x: True if re.search('csv$', x) else False, allRawDataFiles)\n",
    "    sampleFiles = [os.path.join(aSourceDataDirectory, f) for f in sampleFiles]\n",
    "    sampleFiles = sorted(sampleFiles)\n",
    "\n",
    "    sampleSetNames = [re.search('(\\d\\d\\d)', f).group(1) for f in sampleFiles]\n",
    "\n",
    "    annotationFiles = filter(lambda x: True if re.search('ann', x) else False, allRawDataFiles)\n",
    "    annotationFiles = [os.path.join(aSourceDataDirectory, f) for f in annotationFiles]\n",
    "    annotationFiles = sorted(annotationFiles)\n",
    "\n",
    "    annotationMetadata = download_annotation_metadata()\n",
    "    annotationMetadata.to_hdf(aTargetHDF5, 'Annotation_Metadata')\n",
    "\n",
    "    def record_needs_update(aRecordName, aSampleFile, anAnnotationFile, aTargetHDF5):\n",
    "        '''\n",
    "        use an import data frame to keep track of the timestamps on the source files,\n",
    "        Args:\n",
    "        Returns:\n",
    "            None\n",
    "        '''\n",
    "        import time\n",
    "        checksum = '[{0}{1}]'.format(time.ctime(os.path.getmtime(aSampleFile)),\n",
    "                                     time.ctime(os.path.getmtime(anAnnotationFile)))\n",
    "        try:\n",
    "            try:\n",
    "                record = pd.read_hdf(aTargetHDF5, 'Import_checksums')\n",
    "            except KeyError as e:\n",
    "                record = pd.DataFrame({'record_name': aRecordName, 'checksum': checksum}, index=[0])\n",
    "                pass\n",
    "            else:\n",
    "                checksum_ = record[record['record_name'] == aRecordName].checksum.tolist()[0]\n",
    "                if checksum == checksum_:\n",
    "                    return False\n",
    "\n",
    "        except IndexError:\n",
    "            record = record.append({'record_name': aRecordName, 'checksum': checksum}, ignore_index=True)\n",
    "        else:\n",
    "            record.ix[record.record_name == aRecordName, 'checksum'] = checksum\n",
    "\n",
    "        record.to_hdf(aTargetHDF5, 'Import_checksums')\n",
    "        return True\n",
    "    def import_samples_and_annotations(aSampleCsvFile, anAnnotationTxtFile, aMetadataSet):\n",
    "        from datetime import datetime\n",
    "        \n",
    "        def import_sample_data(aSampleFile):\n",
    "            '''\n",
    "            Args:\n",
    "                aSampleFile :  CSV PHYSIONET data\n",
    "\n",
    "            Returns:\n",
    "                Pandas data frame\n",
    "            '''\n",
    "\n",
    "            dataframe = pd.read_csv(aSampleFile, skiprows=2)  # , encoding='utf-16', header=None)\n",
    "            dataframe.columns = ['Elapsed_Microseconds', 'MLII_milliVolts', 'V5_milliVolts']\n",
    "            dataframe.reset_index(drop=True, inplace=True)\n",
    "            dataframe = dataframe.ix[1:]  # or, skip above\n",
    "            dataframe.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            # Set data types\n",
    "            dataframe.MLII_milliVolts = dataframe.MLII_milliVolts.astype(float)\n",
    "            dataframe.V5_milliVolts = dataframe.V5_milliVolts.astype(float)\n",
    "\n",
    "            # Change the time to a zero base and apply as the index of the data frame\n",
    "            baseTime = datetime.strptime('00:00.00', '%M:%S.%f')\n",
    "            dataframe.index = dataframe.Elapsed_Microseconds.apply(lambda x: datetime.strptime(x[1:-1], '%M:%S.%f') - baseTime)\n",
    "            dataframe.drop('Elapsed_Microseconds', axis=1, inplace=True)\n",
    "\n",
    "            return dataframe\n",
    "\n",
    "\n",
    "        def import_annotation_data(anAnnotationFile):\n",
    "            '''\n",
    "            Args:\n",
    "                anAnnotationFile :\n",
    "\n",
    "            Returns:\n",
    "                DataFrame\n",
    "            '''\n",
    "            dataframe = pd.read_table(anAnnotationFile, sep='\\s\\s+|\\t| C')\n",
    "            dataframe.columns = ['Elapsed_Microseconds', 'Sample_num', 'Type', 'Sub', 'Chan', 'Num', 'Aux']\n",
    "\n",
    "            # Change the time to a zero base and apply as the index of the data frame\n",
    "            baseTime = datetime.strptime('00:00.00', '%M:%S.%f')\n",
    "            dataframe.index = dataframe.Elapsed_Microseconds.apply(lambda x: datetime.strptime(x, '%M:%S.%f') - baseTime)\n",
    "            dataframe.drop('Elapsed_Microseconds', axis=1, inplace=True)\n",
    "\n",
    "            dataframe.Sample_num = dataframe.Sample_num.astype(int)\n",
    "            dataframe.Sub = dataframe.Sub.astype(int)\n",
    "            dataframe.Chan = dataframe.Chan.astype(int)\n",
    "            dataframe.Num = dataframe.Num.astype(int)\n",
    "\n",
    "            return dataframe\n",
    "\n",
    "        '''\n",
    "        Args:\n",
    "            aSampleCsvFile : MIT-BIH file\n",
    "            anAnnotationTxtFile :\n",
    "\n",
    "        Returns:\n",
    "            Pandas data frame\n",
    "        '''\n",
    "\n",
    "        # load samples and annotations, then merge them using timestamp into a single frame\n",
    "        sampleDataFrame = import_sample_data(aSampleCsvFile)\n",
    "        annotationDataFrame = import_annotation_data(anAnnotationTxtFile)\n",
    "        df = pd.concat([sampleDataFrame, annotationDataFrame], axis=1)\n",
    "\n",
    "        #  Convert Type and Aux to integer values\n",
    "        # Labels from MIT-BIH site\n",
    "        arrythmiaSymbols = aMetadataSet[aMetadataSet.arrythmia].Symbol.tolist()\n",
    "        normalSymbols = ['N', 'L', 'R']\n",
    "\n",
    "        labels = ['arrythmia', 'normal']\n",
    "        symbolSets = [arrythmiaSymbols, normalSymbols]\n",
    "        annotationList = ['Type', 'Aux']\n",
    "\n",
    "        for label, symbols in zip(labels, symbolSets):\n",
    "\n",
    "            eventName = label + '_events'\n",
    "            df[eventName] = 0\n",
    "\n",
    "            for annotation in annotationList:\n",
    "                df[eventName] = df.apply(lambda x: 1 if x[annotation] in symbols else x[eventName], axis=1)\n",
    "\n",
    "        # add up each, just to see...\n",
    "        print('calculating event occurances...')\n",
    "        for label in labels:\n",
    "            numEvents = len(df[df[label + '_events'] == 1].index)\n",
    "            print('{0} {1} events'.format(numEvents, label))\n",
    "\n",
    "        return df\n",
    "\n",
    "    for sampleSet, sampleFile, annotationFile in zip(sampleSetNames, sampleFiles, annotationFiles):\n",
    "\n",
    "        recordName = 'Record_' + sampleSet\n",
    "\n",
    "        # skip if we've already imported this data\n",
    "        if not record_needs_update(recordName, sampleFile, annotationFile, aTargetHDF5):\n",
    "            continue\n",
    "\n",
    "        print(recordName)\n",
    "        combinedDataFrame = import_samples_and_annotations(sampleFile, annotationFile, annotationMetadata)\n",
    "        combinedDataFrame.to_hdf(aTargetHDF5, recordName)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_all_data(aDbName, aDbDir, aStagingDir, anHdfDir, anHdfFilename):\n",
    "    '''\n",
    "    a big loader, all routines are incremental, so you can kill and restart\n",
    "\n",
    "    Args:\n",
    "        aMitDbDir \t\t: raw data goes in this directory\n",
    "        aStagingDir \t: the raw data is converted to csv & txt here\n",
    "        anHdfDir \t\t: where the HDF is stored\n",
    "        anHdfFilename \t: fullpath (i know...) to HDF5 file\n",
    "\n",
    "    Returns:\n",
    "        DataFrame\n",
    "    '''\n",
    "    setup_data_directories(aDbDir, aStagingDir, anHdfDir)\n",
    "    \n",
    "    download_physionet_files(aDbName, aDbDir)\n",
    "    convert_physionet_data_to_csv(aDbName, aDbDir, aStagingDir)\n",
    "    build_hdf5_data_store(aStagingDir, anHdfFilename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mitdb_dir = 'data/raw_data'\n",
    "stage_dir = 'data/stage_data'\n",
    "hdf_dir = 'data/hdf5'\n",
    "hdf_filename = 'data/hdf5/mit-bih.hdf'\n",
    "\n",
    "try:\n",
    "    os.mkdir('data')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "load_all_data('mitdb', mitdb_dir, stage_dir, hdf_dir, hdf_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
