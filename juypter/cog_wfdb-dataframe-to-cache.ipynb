{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data from web and convert to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): lxml in /opt/conda/lib/python3.5/site-packages\r\n",
      "\u001b[33mYou are using pip version 8.1.2, however version 9.0.1 is available.\r\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "%run \"cog-web-to-dataframe.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install python tables (for pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): tables in /opt/conda/lib/python3.5/site-packages\r\n",
      "Requirement already satisfied (use --upgrade to upgrade): six in /opt/conda/lib/python3.5/site-packages (from tables)\r\n",
      "Requirement already satisfied (use --upgrade to upgrade): numexpr>=2.5.2 in /opt/conda/lib/python3.5/site-packages (from tables)\r\n",
      "Requirement already satisfied (use --upgrade to upgrade): numpy>=1.8.0 in /opt/conda/lib/python3.5/site-packages (from tables)\r\n",
      "\u001b[33mYou are using pip version 8.1.2, however version 9.0.1 is available.\r\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Data Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "hdf_filename = 'data/hdf5/mit-bih.hdf'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_normal_and_arrythmia_samples(anECGDataFrame, anOffsetWindow=[5000, 5000]):\n",
    "    '''\n",
    "\n",
    "    Args:\n",
    "        anECGDataFrame : mitdb data\n",
    "        anOffsetWindow\n",
    "\n",
    "    Returns:\n",
    "        DataFrame containing features for \n",
    "    '''\n",
    "    def generate_time_interval(aFormatString=None, hours=0, minutes=0, seconds=0, microseconds=0):\n",
    "        '''\n",
    "        Args:\n",
    "            aFormatString\n",
    "            hours \t\t\t: number of hours\n",
    "            minutes \t\t: number of minutes\n",
    "            seconds \t\t: number of seconds\n",
    "            mmicroseconds \t: number of mmicroseconds\n",
    "\n",
    "        Returns:\n",
    "            time intevrval\n",
    "        '''\n",
    "        if not aFormatString:\n",
    "            aFormatString = ':'.join(map(str, [hours, minutes, seconds])) + '.' + str(microseconds)\n",
    "\n",
    "        return datetime.strptime(aFormatString, '%H:%M:%S.%f') - datetime.strptime('0:0:0.0', '%H:%M:%S.%f')\n",
    "    def generate_sample_intervals(aDataFrame, aTimeIndex, aLabel, aStartInterval, anEndInterval,\n",
    "                                  aColumnList):\n",
    "        '''\n",
    "        Args:\n",
    "            aDataFrame : time series data\n",
    "            aTimeIndex : index of events in data\n",
    "            aLabel : class to associate with these events\n",
    "            aStartInterval : how far back to go \n",
    "\n",
    "        Returns:\n",
    "            DataFrame\n",
    "        '''\n",
    "        def max_amplitude_filter(aSingleChannel):\n",
    "            '''\n",
    "            Args:\n",
    "                aSingleChannel  :\t  DataFrame\n",
    "\n",
    "            Returns:\n",
    "                filtered (list): temporal list of filtered values, peaks accentuated\n",
    "            '''\n",
    "            #modeFit = pd.rolling_kurt(aSingleChannel, 100)\n",
    "            modeFit = aSingleChannel.rolling(window=100,center=False).kurt()\n",
    "            #stdDev = pd.rolling_std(aSingleChannel - pd.rolling_mean(aSingleChannel, 10), 10)\n",
    "            stdDev = (aSingleChannel - aSingleChannel.rolling(window=10,center=False).mean()).rolling(window=10,center=False).std()\n",
    "            \n",
    "            \n",
    "            return aSingleChannel * modeFit * stdDev\n",
    "\n",
    "        # for each event, generate an interval around it\n",
    "        startIntervalList = aTimeIndex - aStartInterval\n",
    "        endIntervalList = aTimeIndex + anEndInterval\n",
    "\n",
    "        intervals = list(zip(startIntervalList, endIntervalList))\n",
    "        sampleIntervals = []\n",
    "\n",
    "        # for each event interval, save off series data and also features\n",
    "        for start, end in intervals[1:]:\n",
    "\n",
    "            # all mV values in a single series\n",
    "            intervalSamples = aDataFrame.loc[start:end, aColumnList]\n",
    "            intervalSeries = pd.Series(intervalSamples.as_matrix().ravel())\n",
    "\n",
    "            # enhance features in each lead series\n",
    "            # save off the max and var\n",
    "            for signalName in aColumnList:\n",
    "                lead_filtered = max_amplitude_filter(intervalSamples[signalName])\n",
    "                intervalSeries[signalName + '_max'] = lead_filtered.max()\n",
    "                intervalSeries[signalName + '_var'] = lead_filtered.var()\n",
    "\n",
    "            intervalSeries['labels'] = aLabel\n",
    "\n",
    "            sampleIntervals.append(intervalSeries)\n",
    "\n",
    "        return pd.DataFrame(sampleIntervals)\n",
    "\n",
    "\n",
    "    windowStartOffset = anOffsetWindow[0]\n",
    "    windowEndOffset = anOffsetWindow[1]\n",
    "    startInterval = generate_time_interval(microseconds=windowStartOffset)\n",
    "    endInterval = generate_time_interval(microseconds=windowEndOffset)\n",
    "\n",
    "    normalIndex = anECGDataFrame[anECGDataFrame.normal_events == 1].index\n",
    "    arrythmiaIndex = anECGDataFrame[anECGDataFrame.arrythmia_events == 1].index\n",
    "\n",
    "    leadNames = ['MLII_milliVolts', 'V5_milliVolts']\n",
    "    normalFeatures    = generate_sample_intervals(anECGDataFrame, normalIndex, 0, startInterval, endInterval, leadNames)\n",
    "    arrythmiaFeatures = generate_sample_intervals(anECGDataFrame, arrythmiaIndex, 1, startInterval, endInterval, leadNames)\n",
    "\n",
    "    # combine data frames\n",
    "    if arrythmiaFeatures.shape[0] > 0:\n",
    "        normalFeatures = pd.concat([normalFeatures, arrythmiaFeatures])\n",
    "\n",
    "    return normalFeatures\n",
    "\n",
    "def generate_all_sample_record_intervals(anEcgDataFrame, anEqualSampling=True):\n",
    "    '''\n",
    "    grab records from HDS5 Datastore, process into huge set of feature vectors\n",
    "\n",
    "    Args:\n",
    "        anEcgDataFrame : mitDB data\n",
    "        anEqualSampling : make equal number of each class\n",
    "\n",
    "    Returns:\n",
    "        DataFrame\n",
    "    '''\n",
    "\n",
    "    # get a list of all the recordings\n",
    "    ecgFilter = filter(lambda x: re.search('Record_', x), anEcgDataFrame.keys())\n",
    "    ecgDataFrames = [anEcgDataFrame[k] for k in ecgFilter]  # replace with equery\n",
    "\n",
    "    # generate all the sample data intervals for each record that has arrythmias\n",
    "    # each record will have time around each annotated event\n",
    "    # also, derived features are around each event\n",
    "    mlStage = pd.DataFrame()\n",
    "    for record in ecgDataFrames:\n",
    "        if record.arrythmia_events.sum() > 1:\n",
    "            if len(record[record.arrythmia_events == 1].index) == 0:\n",
    "                continue\n",
    "\n",
    "            recordSamples = generate_normal_and_arrythmia_samples(record)\n",
    "            mlStage = pd.concat([mlStage, recordSamples])\n",
    "\n",
    "    mlStage.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    if len(mlStage.index) == 0:\n",
    "        print(\"no arrythmia records, nothing to learn...\")\n",
    "        return\n",
    "\n",
    "    ''' Reduce the number of normal samples to match the number of arrithmia samples\n",
    "    '''\n",
    "    if anEqualSampling:\n",
    "        mask = mlStage['labels'] == 1  # 1 = arrythmia\n",
    "        size = mlStage[mask].shape[0]  # total arrythmias\n",
    "        randNormIndex = np.random.choice(mlStage[~mask].index, size)  # grab random normal\n",
    "        index = np.concatenate([randNormIndex, mlStage[mask].index])  # \n",
    "        mlStage = mlStage.ix[index]\n",
    "        mlStage.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return mlStage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create cached dataset if it doesn't exist, otherwise process it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached ML data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/pandas/io/pytables.py:270: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed-integer,key->axis0] [items->None]\n",
      "\n",
      "  f(store)\n",
      "/opt/conda/lib/python3.5/site-packages/pandas/io/pytables.py:270: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed-integer,key->block0_items] [items->None]\n",
      "\n",
      "  f(store)\n"
     ]
    }
   ],
   "source": [
    "def extract_and_stage_ml(hdf_filename, anEqualSampling=True, useCached=True):\n",
    "    '''\n",
    "    Query the file for the arrythmia data, bring in normal data too\n",
    "\n",
    "    Args:\n",
    "        anEcgDataFrame\t\t: data frame\n",
    "        anEqualSampling\t\t: equalize the amount of each class\n",
    "\n",
    "    Returns:\n",
    "        DataFrame\n",
    "    '''\n",
    "    cache = 'cached_eq_ml_data.hdf'\n",
    "    if useCached and os.path.isfile(cache):\n",
    "        equalized_data = pd.read_hdf(cache, 'cached_data')\n",
    "        print('Using cached ML data...')\n",
    "    else:\n",
    "        ecgDataframe = pd.HDFStore(hdf_filename)\n",
    "        equalized_data = generate_all_sample_record_intervals(ecgDataframe)\n",
    "        if (useCached):\n",
    "            print('Creating cached ML data...')\n",
    "            equalized_data.to_hdf(cache, 'cached_data')\n",
    "\n",
    "    return equalized_data\n",
    "\n",
    "samples = extract_and_stage_ml(hdf_filename, anEqualSampling=True, useCached=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
